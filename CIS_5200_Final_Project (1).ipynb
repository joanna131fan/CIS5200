{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH90-hH5mcEf"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6xPn_PCS4-V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1UnGzb5M8ew"
      },
      "source": [
        "# Fetch data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX3hAGRKNU7_"
      },
      "source": [
        "First, upload your kaggle.json file:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqJtgt0wNdOU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "outputId": "feaf6509-d5f3-4dcf-b980-c7048985922a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ffaed110-dd3d-490c-bbf4-3b686da037b0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ffaed110-dd3d-490c-bbf4-3b686da037b0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (2).json\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "# NOTE: having files downloaded here instead of fetching from Drive is much faster to read\n",
        "from google.colab import files \n",
        "files.upload()\n",
        "\n",
        "! mkdir ~/.kaggle \n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYS3MYgUK_Ln"
      },
      "source": [
        "We now download the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4mGUa1pKre6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c0be5b0-47e7-4144-f47f-aeacac067bc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gtsrb-german-traffic-sign.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "! kaggle datasets download -d meowmeowmeowmeowmeow/gtsrb-german-traffic-sign"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw-Z9d1-Ln-s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9233c6b-7838-4fe8-f02b-f1240fdc9a02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  gtsrb-german-traffic-sign.zip\n",
            "replace data/Meta.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip gtsrb-german-traffic-sign -d data\n",
        "\n",
        "data_path = \"./data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58mTmGMETouT"
      },
      "source": [
        "# GTSRBDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDKsEmwJO18R"
      },
      "source": [
        "Our GTSRB dataset is a bit different from the GTSRB dataset available from PyTorch (**TODO: EXPLAIN HOW **)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUzBK5TfO4jZ"
      },
      "source": [
        "To make working with the data easier, we will wrap it in a GTSRBDataset class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wby0IKnPA0r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "class GTSRBDataset(Dataset):\n",
        "  def __init__(self, root, train, transform=None, target_transform=None):\n",
        "    img_path = \"Train\" if train else \"Test\"\n",
        "    labels_path = \"Train.csv\" if train else \"Test.csv\"\n",
        "\n",
        "    img_dir_path = os.path.join(root, img_path)\n",
        "    img_labels_path = os.path.join(root, labels_path)\n",
        "\n",
        "    self.root = root\n",
        "\n",
        "    self.img_labels = pd.read_csv(img_labels_path)\n",
        "    self.img_dir = img_dir_path\n",
        "\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = self.img_labels.iloc[idx, 7]\n",
        "    img_path = os.path.join(self.root, img_path)\n",
        "\n",
        "    image = read_image(img_path)\n",
        "\n",
        "    label = self.img_labels.iloc[idx, 6]\n",
        "\n",
        "    if self.transform is not None:\n",
        "      image = self.transform(image)\n",
        "    if self.target_transform is not None:\n",
        "      label = self.target_transform(label)\n",
        "\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMpzakC_TYuG"
      },
      "source": [
        "# Training, Validation, and Testing datasets, and Transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zg0857OWsMd"
      },
      "source": [
        "Let's first define the Transforms for our data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUTdR-MyDr0k"
      },
      "outputs": [],
      "source": [
        "data_mean = np.array([0.3337, 0.3064, 0.3171])\n",
        "data_std = np.array([0.2672, 0.2564, 0.2629])\n",
        "\n",
        "# Resize all images to 32 * 32 and normalize them\n",
        "data_transforms = transforms.Compose([\n",
        "  transforms.ToPILImage(),\n",
        "  transforms.Resize([32, 32]),\n",
        "  transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0),\n",
        "  ToTensor(),\n",
        "  transforms.Normalize(data_mean, data_std)\n",
        "])\n",
        "\n",
        "data_transforms_rpt = transforms.Compose([\n",
        "  transforms.ToPILImage(),\n",
        "  transforms.Resize([224, 224]),\n",
        "  transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0),\n",
        "  ToTensor(),\n",
        "  transforms.Normalize(data_mean, data_std)\n",
        "])\n",
        "\n",
        "data_transforms_inc = transforms.Compose([\n",
        "  transforms.ToPILImage(),\n",
        "  transforms.Resize([299, 299]),\n",
        "  transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0),\n",
        "  ToTensor(),\n",
        "  transforms.Normalize(data_mean, data_std)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QWRSpK9TiUt"
      },
      "source": [
        "Now, let's create our training and test set. Then, we split the training set up into training and validation sets with 80% of the data being randomly assigned to the training set and the rest being assigned to the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PVb-d5DT8cQ"
      },
      "outputs": [],
      "source": [
        "trainset = GTSRBDataset(\n",
        "    root=data_path,\n",
        "    train=True,\n",
        "    transform=data_transforms,\n",
        ")\n",
        "testset = GTSRBDataset(\n",
        "    root=data_path,\n",
        "    train=False,\n",
        "    transform=data_transforms,\n",
        ")\n",
        "\n",
        "trainset_rpt = GTSRBDataset(\n",
        "    root=data_path,\n",
        "    train=True,\n",
        "    transform=data_transforms_rpt,\n",
        ")\n",
        "testset_rpt = GTSRBDataset(\n",
        "    root=data_path,\n",
        "    train=False,\n",
        "    transform=data_transforms_rpt,\n",
        ")\n",
        "\n",
        "trainset_inc = GTSRBDataset(\n",
        "    root=data_path,\n",
        "    train=True,\n",
        "    transform=data_transforms_inc,\n",
        ")\n",
        "testset_inc = GTSRBDataset(\n",
        "    root=data_path,\n",
        "    train=False,\n",
        "    transform=data_transforms_inc,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG3ZKJWIUSdR"
      },
      "outputs": [],
      "source": [
        "train_size = len(trainset)\n",
        "print(f\"Size of training set before split: {train_size}\\n\")\n",
        "\n",
        "split_ratio = 0.8\n",
        "new_train_size = int(train_size * split_ratio)\n",
        "print(f\"Size of training set after split: {new_train_size}\\n\")\n",
        "\n",
        "val_size = train_size - new_train_size\n",
        "print(f\"Size of validation set: {val_size}\\n\")\n",
        "\n",
        "test_size = len(testset)\n",
        "print(f\"Size of testing set: {test_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYzmkNuvk-t_"
      },
      "outputs": [],
      "source": [
        "trainset, valset = torch.utils.data.random_split(trainset, [new_train_size, val_size])\n",
        "trainset_rpt, valset_rpt = torch.utils.data.random_split(trainset_rpt, [new_train_size, val_size])\n",
        "trainset_inc, valset_inc = torch.utils.data.random_split(trainset_inc, [new_train_size, val_size])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIqvNEx4VoKj"
      },
      "source": [
        "# Visualizing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iQPH4xFV8R9"
      },
      "source": [
        "The possible labels/classes of our data are the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNIXcMVCej4W"
      },
      "outputs": [],
      "source": [
        "classes = { 0:'Speed limit (20km/h)',\n",
        "            1:'Speed limit (30km/h)', \n",
        "            2:'Speed limit (50km/h)', \n",
        "            3:'Speed limit (60km/h)', \n",
        "            4:'Speed limit (70km/h)', \n",
        "            5:'Speed limit (80km/h)', \n",
        "            6:'End of speed limit (80km/h)', \n",
        "            7:'Speed limit (100km/h)', \n",
        "            8:'Speed limit (120km/h)', \n",
        "            9:'No passing', \n",
        "            10:'No passing veh over 3.5 tons', \n",
        "            11:'Right-of-way at intersection', \n",
        "            12:'Priority road', \n",
        "            13:'Yield', \n",
        "            14:'Stop', \n",
        "            15:'No vehicles', \n",
        "            16:'Veh > 3.5 tons prohibited', \n",
        "            17:'No entry', \n",
        "            18:'General caution', \n",
        "            19:'Dangerous curve left', \n",
        "            20:'Dangerous curve right', \n",
        "            21:'Double curve', \n",
        "            22:'Bumpy road', \n",
        "            23:'Slippery road', \n",
        "            24:'Road narrows on the right', \n",
        "            25:'Road work', \n",
        "            26:'Traffic signals', \n",
        "            27:'Pedestrians', \n",
        "            28:'Children crossing', \n",
        "            29:'Bicycles crossing', \n",
        "            30:'Beware of ice/snow',\n",
        "            31:'Wild animals crossing', \n",
        "            32:'End speed + passing limits', \n",
        "            33:'Turn right ahead', \n",
        "            34:'Turn left ahead', \n",
        "            35:'Ahead only', \n",
        "            36:'Go straight or right', \n",
        "            37:'Go straight or left', \n",
        "            38:'Keep right', \n",
        "            39:'Keep left', \n",
        "            40:'Roundabout mandatory', \n",
        "            41:'End of no passing', \n",
        "            42:'End no passing vehicles > 3.5 tons' }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l84TavQhV1zA"
      },
      "source": [
        "Let's see the first eight images (before transformation) in the testing set and their corresponding labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DDXV9tGWFW5"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(32, 32))\n",
        "\n",
        "for i in range(8):\n",
        "  fig.add_subplot(1, 8, i+1)\n",
        "\n",
        "  label = testset[i][1]\n",
        "  label = classes[label]\n",
        "  plt.title(label)\n",
        "\n",
        "  img = testset[i][0].numpy()\n",
        "  img = np.transpose(img, (1, 2, 0))\n",
        "  img = img * data_std + data_mean\n",
        "\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNLJ_sE-ZHIw"
      },
      "source": [
        "And now let's visualize the distribution of the Training and Validation data among the labels:\n",
        "Classes are zero-indexed, so for the graph we shifted it over by 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rth-hVU9Ciq9"
      },
      "outputs": [],
      "source": [
        "train_plot = [0]*len(classes)\n",
        "val_plot = [0]*len(classes)\n",
        "combine_plot = [0]*len(classes)\n",
        "\n",
        "for (_, label) in trainset:\n",
        "  train_plot[label] += 1\n",
        "\n",
        "for (_, label) in valset:\n",
        "  val_plot[label] += 1\n",
        "\n",
        "for i in range(43):\n",
        "  combine_plot[i] = train_plot[i]+val_plot[i]\n",
        "\n",
        "plt.bar(range(len(classes)), combine_plot, label=\"train\")\n",
        "plt.bar(range(len(classes)), val_plot, label=\"val\")\n",
        "\n",
        "legend = plt.legend(loc='upper right', shadow=True)\n",
        "plt.title(\"Distribution Plot\")\n",
        "plt.xlabel(\"Class ID\")\n",
        "plt.ylabel(\"# of examples\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1-JQq7eJAeF"
      },
      "outputs": [],
      "source": [
        "test_plot = [0]*len(classes)\n",
        "\n",
        "for (_, label) in testset:\n",
        "  test_plot[label] += 1\n",
        "\n",
        "plt.bar(range(len(classes)), test_plot, label=\"test\")\n",
        "\n",
        "legend = plt.legend(loc='upper right', shadow=True)\n",
        "plt.title(\"Distribution Plot\")\n",
        "plt.xlabel(\"Class ID\")\n",
        "plt.ylabel(\"# of examples\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln3z9vwcc5g9"
      },
      "source": [
        "# LeNet\n",
        "As a representative of the early convolutional neural network, LeNet possesses the basic units of convolutional neural network, such as convolutional layer, pooling layer and full connection layer, laying a foundation for the future development of convolutional neural network. The input is a 32x32 pixel and the entirety of LeNet consists of seven layers. In addition to input, every other layer can train parameters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEGP3CYUxjaq"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "# https://www.kaggle.com/code/blurredmachine/lenet-architecture-a-complete-guide/notebook\n",
        "# replace softmax with relu\n",
        "class LeNet(nn.Module):\n",
        "  def __init__(self, num_classes=43, input_channels=3):\n",
        "    super(LeNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(input_channels, 6, 5)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.fc1 = nn.Linear(16*5*5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, num_classes)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "\n",
        "    x = torch.flatten(x, 1) \n",
        "\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# alternate version: add batch norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1Z96a6bbhLE"
      },
      "outputs": [],
      "source": [
        "# LeNet with Batch Normalization\n",
        "class LeNet_BN(nn.Module):\n",
        "  def __init__(self, num_classes=43, input_channels=3):\n",
        "    super(LeNet_BN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(input_channels, 6, 5)\n",
        "    self.conv1_bn = nn.BatchNorm2d(6)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.conv2_bn = nn.BatchNorm2d(16)\n",
        "    self.fc1 = nn.Linear(16*5*5, 120)\n",
        "    self.fc1_bn = nn.BatchNorm1d(120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc2_bn = nn.BatchNorm1d(84)\n",
        "    self.fc3 = nn.Linear(84, num_classes)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.pool(F.relu(self.conv1_bn(x)))\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.pool(F.relu(self.conv2_bn(x)))\n",
        "\n",
        "    x = torch.flatten(x, 1) \n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(self.fc1_bn(x))\n",
        "\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(self.fc2_bn(x))\n",
        "    \n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz8mp8c5cxfs"
      },
      "source": [
        "# ResNet and SENet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY7TzmBqElHg"
      },
      "outputs": [],
      "source": [
        "middle_layers = [2, 2, 2, 2]\n",
        "middle_layers_channel = [64, 128, 256, 512]\n",
        "middle_layers_prev_channels = [64, 64, 128, 256]\n",
        "\n",
        "# first_layer + 2 * sum(middle_layers) + 1_dense_layer_at_end = 18 layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels, first_stride, transition=None):\n",
        "    super(Block, self).__init__()\n",
        "\n",
        "    self.transition = transition\n",
        "\n",
        "    conv1 = nn.Conv2d(\n",
        "        in_channels=in_channels,\n",
        "        out_channels=hidden_channels,\n",
        "        kernel_size=3,\n",
        "        stride=first_stride,\n",
        "        padding=1)\n",
        "    batch_norm1 = nn.BatchNorm2d(hidden_channels)\n",
        "    conv2 = nn.Conv2d(\n",
        "        in_channels=hidden_channels,\n",
        "        out_channels=out_channels,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1)\n",
        "    batch_norm2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.block = nn.Sequential(\n",
        "        conv1,\n",
        "        batch_norm1,\n",
        "        nn.ReLU(),\n",
        "        conv2,\n",
        "        batch_norm2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    pass"
      ],
      "metadata": {
        "id": "sqPRXWHa1eLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet"
      ],
      "metadata": {
        "id": "CaafGIom1pRn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GBCZQli0HZv"
      },
      "source": [
        "We now implement Residual Neural Networks. We will implement a 18-layer NN without ResNet, and a 18-layer NN with ResNet. This will be similar to the architecture shown in the paper.\n",
        "\n",
        "Note that we won't implement Bottleneck layer blocks, since the models we are training have less than 50 layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsP3iFI0wfq6"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(Block):\n",
        "  def __init__(self, use_resnet, in_channels, hidden_channels, out_channels, first_stride, transition=None):\n",
        "    super(ResidualBlock, self).__init__(in_channels, hidden_channels, out_channels, first_stride, transition)\n",
        "    self.use_resnet = use_resnet\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.use_resnet:\n",
        "      res = x if self.transition is None else self.transition(x)\n",
        "      return F.relu(res + self.block(x))\n",
        "\n",
        "    return F.relu(self.block(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbLsgANKUj-4"
      },
      "outputs": [],
      "source": [
        "class ResNetMiddleLayer(nn.Module):\n",
        "  def __init__(self, use_resnet, block, n_blocks, channels, prev_channels):\n",
        "    super(ResNetMiddleLayer, self).__init__()\n",
        "\n",
        "    first_stride_in_block = 1 if channels == 64 else 2\n",
        "\n",
        "    transition = nn.Sequential(\n",
        "      nn.Conv2d(\n",
        "          in_channels=prev_channels,\n",
        "          out_channels=channels,\n",
        "          kernel_size=1,\n",
        "          stride=2,\n",
        "      ),\n",
        "      nn.BatchNorm2d(channels)) if channels != 64 and use_resnet else None\n",
        "\n",
        "    in_channels_first_block = channels if channels == 64 else prev_channels\n",
        "\n",
        "    first_block = block(\n",
        "      use_resnet=use_resnet,\n",
        "      in_channels=in_channels_first_block,\n",
        "      hidden_channels=channels,\n",
        "      out_channels=channels,\n",
        "      first_stride=first_stride_in_block,\n",
        "      transition=transition)\n",
        "\n",
        "    remaining_blocks = (block(\n",
        "      use_resnet=use_resnet,\n",
        "      in_channels=channels,\n",
        "      hidden_channels=channels,\n",
        "      out_channels=channels,\n",
        "      first_stride=1,\n",
        "    ) for _ in range(n_blocks - 1))\n",
        "\n",
        "    self.layer = nn.Sequential(\n",
        "      first_block,\n",
        "      *remaining_blocks,\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.layer(x)\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self, use_resnet=True, output_classes=43, in_channels=3, block=ResidualBlock):\n",
        "    super(ResNet, self).__init__()\n",
        "\n",
        "    self.first_layer = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=64,\n",
        "            kernel_size=7,\n",
        "            stride=2,\n",
        "            padding=3,\n",
        "        ),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.MaxPool2d(\n",
        "            kernel_size=3,\n",
        "            stride=2,\n",
        "            padding=1,\n",
        "        ),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    self.middle_layers = nn.ModuleList(\n",
        "        [ResNetMiddleLayer(use_resnet, block, n_blocks, channels, middle_layers_prev_channels[i])\n",
        "          for i, (n_blocks, channels) in enumerate(zip(middle_layers, middle_layers_channel))]\n",
        "    )\n",
        "\n",
        "    self.final_layer = nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d(1),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(512, output_classes))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.first_layer(x)\n",
        "\n",
        "    for middle_layer in self.middle_layers:\n",
        "      x = middle_layer(x)\n",
        "\n",
        "    x = self.final_layer(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Squeeze and Excitation Networks"
      ],
      "metadata": {
        "id": "mYlqi0eSGs3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class SENetBlock(nn.Module):\n",
        "  def __init__(self, channels, ratio=8, last_activation=nn.Sigmoid):\n",
        "    super(SENetBlock, self).__init__()\n",
        "    self.senet_block = nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d(1),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(channels, channels // ratio),\n",
        "        nn.ReLU(), \n",
        "        nn.Linear(channels // ratio, channels),\n",
        "        last_activation(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    a, b, _, _ = x.size()\n",
        "    out = self.senet_block(x)\n",
        "\n",
        "    return x * out.view(a, b, 1, 1)"
      ],
      "metadata": {
        "id": "QcgFB6PU5SCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SENet (with ResNet)"
      ],
      "metadata": {
        "id": "OozlEajn5Lvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SEAndResNetBlock_Temp(Block):\n",
        "  def __init__(self, use_resnet, in_channels, hidden_channels, out_channels, first_stride, transition=None, last_activation=nn.Sigmoid):\n",
        "    super(SEAndResNetBlock_Temp, self).__init__(in_channels, hidden_channels, out_channels, first_stride, transition)\n",
        "    self.last_activation = 2\n",
        "    self.use_resnet = use_resnet\n",
        "\n",
        "    self.final_block = nn.Sequential(\n",
        "        self.block,\n",
        "        SENetBlock(channels=out_channels, last_activation=last_activation),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.use_resnet:\n",
        "      res = x if self.transition is None else self.transition(x)\n",
        "      return F.relu(res + self.final_block(x))\n",
        "\n",
        "    return F.relu(self.final_block(x))\n",
        "\n",
        "\n",
        "def SEAndResNetBlock(last_activation):\n",
        "  return functools.partial(SEAndResNetBlock_Temp, last_activation=last_activation)\n",
        "\n",
        "\n",
        "class SEAndResNet(nn.Module):\n",
        "  def __init__(self, use_resnet=True, output_classes=43, in_channels=3, last_activation=nn.Sigmoid):\n",
        "    super(SEAndResNet, self).__init__()\n",
        "    self.senet = ResNet(use_resnet, output_classes, in_channels, SEAndResNetBlock(last_activation))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.senet(x)\n",
        "\n",
        "print(SEAndResNet())\n",
        "summary(SEAndResNet().cuda(), input_size=(3, 32, 32))"
      ],
      "metadata": {
        "id": "wu3wzotKGxZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f91486bb-e9a8-4ba7-a1d8-9f77aebf8b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEAndResNet(\n",
            "  (senet): ResNet(\n",
            "    (first_layer): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (3): ReLU()\n",
            "    )\n",
            "    (middle_layers): ModuleList(\n",
            "      (0): ResNetMiddleLayer(\n",
            "        (layer): Sequential(\n",
            "          (0): SEAndResNetBlock_Temp(\n",
            "            (relu): ReLU()\n",
            "            (block): Sequential(\n",
            "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU()\n",
            "              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (final_block): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                (2): ReLU()\n",
            "                (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "              (1): SENetBlock(\n",
            "                (senet_block): Sequential(\n",
            "                  (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                  (1): Flatten(start_dim=1, end_dim=-1)\n",
            "                  (2): Linear(in_features=64, out_features=8, bias=True)\n",
            "                  (3): ReLU()\n",
            "                  (4): Linear(in_features=8, out_features=64, bias=True)\n",
            "                  (5): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (1): SEAndResNetBlock_Temp(\n",
            "            (relu): ReLU()\n",
            "            (block): Sequential(\n",
            "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU()\n",
            "              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (final_block): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                (2): ReLU()\n",
            "                (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "              (1): SENetBlock(\n",
            "                (senet_block): Sequential(\n",
            "                  (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                  (1): Flatten(start_dim=1, end_dim=-1)\n",
            "                  (2): Linear(in_features=64, out_features=8, bias=True)\n",
            "                  (3): ReLU()\n",
            "                  (4): Linear(in_features=8, out_features=64, bias=True)\n",
            "                  (5): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): ResNetMiddleLayer(\n",
            "        (layer): Sequential(\n",
            "          (0): SEAndResNetBlock_Temp(\n",
            "            (transition): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (relu): ReLU()\n",
            "            (block): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU()\n",
            "              (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (final_block): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                (2): ReLU()\n",
            "                (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "              (1): SENetBlock(\n",
            "                (senet_block): Sequential(\n",
            "                  (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                  (1): Flatten(start_dim=1, end_dim=-1)\n",
            "                  (2): Linear(in_features=128, out_features=16, bias=True)\n",
            "                  (3): ReLU()\n",
            "                  (4): Linear(in_features=16, out_features=128, bias=True)\n",
            "                  (5): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (1): SEAndResNetBlock_Temp(\n",
            "            (relu): ReLU()\n",
            "            (block): Sequential(\n",
            "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU()\n",
            "              (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (final_block): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                (2): ReLU()\n",
            "                (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "              (1): SENetBlock(\n",
            "                (senet_block): Sequential(\n",
            "                  (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                  (1): Flatten(start_dim=1, end_dim=-1)\n",
            "                  (2): Linear(in_features=128, out_features=16, bias=True)\n",
            "                  (3): ReLU()\n",
            "                  (4): Linear(in_features=16, out_features=128, bias=True)\n",
            "                  (5): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (2): ResNetMiddleLayer(\n",
            "        (layer): Sequential(\n",
            "          (0): SEAndResNetBlock_Temp(\n",
            "            (transition): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (relu): ReLU()\n",
            "            (block): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU()\n",
            "              (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (final_block): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                (2): ReLU()\n",
            "                (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "              (1): SENetBlock(\n",
            "                (senet_block): Sequential(\n",
            "                  (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                  (1): Flatten(start_dim=1, end_dim=-1)\n",
            "                  (2): Linear(in_features=256, out_features=32, bias=True)\n",
            "                  (3): ReLU()\n",
            "                  (4): Linear(in_features=32, out_features=256, bias=True)\n",
            "                  (5): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (1): SEAndResNetBlock_Temp(\n",
            "            (relu): ReLU()\n",
            "            (block): Sequential(\n",
            "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU()\n",
            "              (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (final_block): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                (2): ReLU()\n",
            "                (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "              (1): SENetBlock(\n",
            "                (senet_block): Sequential(\n",
            "                  (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                  (1): Flatten(start_dim=1, end_dim=-1)\n",
            "                  (2): Linear(in_features=256, out_features=32, bias=True)\n",
            "                  (3): ReLU()\n",
            "                  (4): Linear(in_features=32, out_features=256, bias=True)\n",
            "                  (5): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (3): ResNetMiddleLayer(\n",
            "        (layer): Sequential(\n",
            "          (0): SEAndResNetBlock_Temp(\n",
            "            (transition): Sequential(\n",
            "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
            "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (relu): ReLU()\n",
            "            (block): Sequential(\n",
            "              (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU()\n",
            "              (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (final_block): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                (2): ReLU()\n",
            "                (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "              (1): SENetBlock(\n",
            "                (senet_block): Sequential(\n",
            "                  (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                  (1): Flatten(start_dim=1, end_dim=-1)\n",
            "                  (2): Linear(in_features=512, out_features=64, bias=True)\n",
            "                  (3): ReLU()\n",
            "                  (4): Linear(in_features=64, out_features=512, bias=True)\n",
            "                  (5): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (1): SEAndResNetBlock_Temp(\n",
            "            (relu): ReLU()\n",
            "            (block): Sequential(\n",
            "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU()\n",
            "              (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (final_block): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                (2): ReLU()\n",
            "                (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "              (1): SENetBlock(\n",
            "                (senet_block): Sequential(\n",
            "                  (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                  (1): Flatten(start_dim=1, end_dim=-1)\n",
            "                  (2): Linear(in_features=512, out_features=64, bias=True)\n",
            "                  (3): ReLU()\n",
            "                  (4): Linear(in_features=64, out_features=512, bias=True)\n",
            "                  (5): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer): Sequential(\n",
            "      (0): AdaptiveAvgPool2d(output_size=1)\n",
            "      (1): Flatten(start_dim=1, end_dim=-1)\n",
            "      (2): Linear(in_features=512, out_features=43, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,472\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "         MaxPool2d-3             [-1, 64, 8, 8]               0\n",
            "              ReLU-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5             [-1, 64, 8, 8]          36,928\n",
            "            Conv2d-6             [-1, 64, 8, 8]          36,928\n",
            "       BatchNorm2d-7             [-1, 64, 8, 8]             128\n",
            "       BatchNorm2d-8             [-1, 64, 8, 8]             128\n",
            "              ReLU-9             [-1, 64, 8, 8]               0\n",
            "             ReLU-10             [-1, 64, 8, 8]               0\n",
            "           Conv2d-11             [-1, 64, 8, 8]          36,928\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "      BatchNorm2d-14             [-1, 64, 8, 8]             128\n",
            "AdaptiveAvgPool2d-15             [-1, 64, 1, 1]               0\n",
            "          Flatten-16                   [-1, 64]               0\n",
            "           Linear-17                    [-1, 8]             520\n",
            "             ReLU-18                    [-1, 8]               0\n",
            "           Linear-19                   [-1, 64]             576\n",
            "          Sigmoid-20                   [-1, 64]               0\n",
            "       SENetBlock-21             [-1, 64, 8, 8]               0\n",
            "SEAndResNetBlock_Temp-22             [-1, 64, 8, 8]               0\n",
            "           Conv2d-23             [-1, 64, 8, 8]          36,928\n",
            "           Conv2d-24             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-25             [-1, 64, 8, 8]             128\n",
            "      BatchNorm2d-26             [-1, 64, 8, 8]             128\n",
            "             ReLU-27             [-1, 64, 8, 8]               0\n",
            "             ReLU-28             [-1, 64, 8, 8]               0\n",
            "           Conv2d-29             [-1, 64, 8, 8]          36,928\n",
            "           Conv2d-30             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-31             [-1, 64, 8, 8]             128\n",
            "      BatchNorm2d-32             [-1, 64, 8, 8]             128\n",
            "AdaptiveAvgPool2d-33             [-1, 64, 1, 1]               0\n",
            "          Flatten-34                   [-1, 64]               0\n",
            "           Linear-35                    [-1, 8]             520\n",
            "             ReLU-36                    [-1, 8]               0\n",
            "           Linear-37                   [-1, 64]             576\n",
            "          Sigmoid-38                   [-1, 64]               0\n",
            "       SENetBlock-39             [-1, 64, 8, 8]               0\n",
            "SEAndResNetBlock_Temp-40             [-1, 64, 8, 8]               0\n",
            "ResNetMiddleLayer-41             [-1, 64, 8, 8]               0\n",
            "           Conv2d-42            [-1, 128, 4, 4]           8,320\n",
            "      BatchNorm2d-43            [-1, 128, 4, 4]             256\n",
            "           Conv2d-44            [-1, 128, 4, 4]          73,856\n",
            "           Conv2d-45            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-46            [-1, 128, 4, 4]             256\n",
            "      BatchNorm2d-47            [-1, 128, 4, 4]             256\n",
            "             ReLU-48            [-1, 128, 4, 4]               0\n",
            "             ReLU-49            [-1, 128, 4, 4]               0\n",
            "           Conv2d-50            [-1, 128, 4, 4]         147,584\n",
            "           Conv2d-51            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-52            [-1, 128, 4, 4]             256\n",
            "      BatchNorm2d-53            [-1, 128, 4, 4]             256\n",
            "AdaptiveAvgPool2d-54            [-1, 128, 1, 1]               0\n",
            "          Flatten-55                  [-1, 128]               0\n",
            "           Linear-56                   [-1, 16]           2,064\n",
            "             ReLU-57                   [-1, 16]               0\n",
            "           Linear-58                  [-1, 128]           2,176\n",
            "          Sigmoid-59                  [-1, 128]               0\n",
            "       SENetBlock-60            [-1, 128, 4, 4]               0\n",
            "SEAndResNetBlock_Temp-61            [-1, 128, 4, 4]               0\n",
            "           Conv2d-62            [-1, 128, 4, 4]         147,584\n",
            "           Conv2d-63            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-64            [-1, 128, 4, 4]             256\n",
            "      BatchNorm2d-65            [-1, 128, 4, 4]             256\n",
            "             ReLU-66            [-1, 128, 4, 4]               0\n",
            "             ReLU-67            [-1, 128, 4, 4]               0\n",
            "           Conv2d-68            [-1, 128, 4, 4]         147,584\n",
            "           Conv2d-69            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-70            [-1, 128, 4, 4]             256\n",
            "      BatchNorm2d-71            [-1, 128, 4, 4]             256\n",
            "AdaptiveAvgPool2d-72            [-1, 128, 1, 1]               0\n",
            "          Flatten-73                  [-1, 128]               0\n",
            "           Linear-74                   [-1, 16]           2,064\n",
            "             ReLU-75                   [-1, 16]               0\n",
            "           Linear-76                  [-1, 128]           2,176\n",
            "          Sigmoid-77                  [-1, 128]               0\n",
            "       SENetBlock-78            [-1, 128, 4, 4]               0\n",
            "SEAndResNetBlock_Temp-79            [-1, 128, 4, 4]               0\n",
            "ResNetMiddleLayer-80            [-1, 128, 4, 4]               0\n",
            "           Conv2d-81            [-1, 256, 2, 2]          33,024\n",
            "      BatchNorm2d-82            [-1, 256, 2, 2]             512\n",
            "           Conv2d-83            [-1, 256, 2, 2]         295,168\n",
            "           Conv2d-84            [-1, 256, 2, 2]         295,168\n",
            "      BatchNorm2d-85            [-1, 256, 2, 2]             512\n",
            "      BatchNorm2d-86            [-1, 256, 2, 2]             512\n",
            "             ReLU-87            [-1, 256, 2, 2]               0\n",
            "             ReLU-88            [-1, 256, 2, 2]               0\n",
            "           Conv2d-89            [-1, 256, 2, 2]         590,080\n",
            "           Conv2d-90            [-1, 256, 2, 2]         590,080\n",
            "      BatchNorm2d-91            [-1, 256, 2, 2]             512\n",
            "      BatchNorm2d-92            [-1, 256, 2, 2]             512\n",
            "AdaptiveAvgPool2d-93            [-1, 256, 1, 1]               0\n",
            "          Flatten-94                  [-1, 256]               0\n",
            "           Linear-95                   [-1, 32]           8,224\n",
            "             ReLU-96                   [-1, 32]               0\n",
            "           Linear-97                  [-1, 256]           8,448\n",
            "          Sigmoid-98                  [-1, 256]               0\n",
            "       SENetBlock-99            [-1, 256, 2, 2]               0\n",
            "SEAndResNetBlock_Temp-100            [-1, 256, 2, 2]               0\n",
            "          Conv2d-101            [-1, 256, 2, 2]         590,080\n",
            "          Conv2d-102            [-1, 256, 2, 2]         590,080\n",
            "     BatchNorm2d-103            [-1, 256, 2, 2]             512\n",
            "     BatchNorm2d-104            [-1, 256, 2, 2]             512\n",
            "            ReLU-105            [-1, 256, 2, 2]               0\n",
            "            ReLU-106            [-1, 256, 2, 2]               0\n",
            "          Conv2d-107            [-1, 256, 2, 2]         590,080\n",
            "          Conv2d-108            [-1, 256, 2, 2]         590,080\n",
            "     BatchNorm2d-109            [-1, 256, 2, 2]             512\n",
            "     BatchNorm2d-110            [-1, 256, 2, 2]             512\n",
            "AdaptiveAvgPool2d-111            [-1, 256, 1, 1]               0\n",
            "         Flatten-112                  [-1, 256]               0\n",
            "          Linear-113                   [-1, 32]           8,224\n",
            "            ReLU-114                   [-1, 32]               0\n",
            "          Linear-115                  [-1, 256]           8,448\n",
            "         Sigmoid-116                  [-1, 256]               0\n",
            "      SENetBlock-117            [-1, 256, 2, 2]               0\n",
            "SEAndResNetBlock_Temp-118            [-1, 256, 2, 2]               0\n",
            "ResNetMiddleLayer-119            [-1, 256, 2, 2]               0\n",
            "          Conv2d-120            [-1, 512, 1, 1]         131,584\n",
            "     BatchNorm2d-121            [-1, 512, 1, 1]           1,024\n",
            "          Conv2d-122            [-1, 512, 1, 1]       1,180,160\n",
            "          Conv2d-123            [-1, 512, 1, 1]       1,180,160\n",
            "     BatchNorm2d-124            [-1, 512, 1, 1]           1,024\n",
            "     BatchNorm2d-125            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-126            [-1, 512, 1, 1]               0\n",
            "            ReLU-127            [-1, 512, 1, 1]               0\n",
            "          Conv2d-128            [-1, 512, 1, 1]       2,359,808\n",
            "          Conv2d-129            [-1, 512, 1, 1]       2,359,808\n",
            "     BatchNorm2d-130            [-1, 512, 1, 1]           1,024\n",
            "     BatchNorm2d-131            [-1, 512, 1, 1]           1,024\n",
            "AdaptiveAvgPool2d-132            [-1, 512, 1, 1]               0\n",
            "         Flatten-133                  [-1, 512]               0\n",
            "          Linear-134                   [-1, 64]          32,832\n",
            "            ReLU-135                   [-1, 64]               0\n",
            "          Linear-136                  [-1, 512]          33,280\n",
            "         Sigmoid-137                  [-1, 512]               0\n",
            "      SENetBlock-138            [-1, 512, 1, 1]               0\n",
            "SEAndResNetBlock_Temp-139            [-1, 512, 1, 1]               0\n",
            "          Conv2d-140            [-1, 512, 1, 1]       2,359,808\n",
            "          Conv2d-141            [-1, 512, 1, 1]       2,359,808\n",
            "     BatchNorm2d-142            [-1, 512, 1, 1]           1,024\n",
            "     BatchNorm2d-143            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-144            [-1, 512, 1, 1]               0\n",
            "            ReLU-145            [-1, 512, 1, 1]               0\n",
            "          Conv2d-146            [-1, 512, 1, 1]       2,359,808\n",
            "          Conv2d-147            [-1, 512, 1, 1]       2,359,808\n",
            "     BatchNorm2d-148            [-1, 512, 1, 1]           1,024\n",
            "     BatchNorm2d-149            [-1, 512, 1, 1]           1,024\n",
            "AdaptiveAvgPool2d-150            [-1, 512, 1, 1]               0\n",
            "         Flatten-151                  [-1, 512]               0\n",
            "          Linear-152                   [-1, 64]          32,832\n",
            "            ReLU-153                   [-1, 64]               0\n",
            "          Linear-154                  [-1, 512]          33,280\n",
            "         Sigmoid-155                  [-1, 512]               0\n",
            "      SENetBlock-156            [-1, 512, 1, 1]               0\n",
            "SEAndResNetBlock_Temp-157            [-1, 512, 1, 1]               0\n",
            "ResNetMiddleLayer-158            [-1, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-159            [-1, 512, 1, 1]               0\n",
            "         Flatten-160                  [-1, 512]               0\n",
            "          Linear-161                   [-1, 43]          22,059\n",
            "          ResNet-162                   [-1, 43]               0\n",
            "================================================================\n",
            "Total params: 22,376,603\n",
            "Trainable params: 22,376,603\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.90\n",
            "Params size (MB): 85.36\n",
            "Estimated Total Size (MB): 87.27\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrwD0JBaMcbx"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWq2gx_hH8IN"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 256\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2QEElpiciwc"
      },
      "source": [
        "# Define a Loss Function + Optimizer\n",
        "For optimizer, we opted to use **Adam** because it combines the advantages of two other extensions of classic stochastic gradient descent. Specifically, \n",
        "\n",
        "- **Adaptive Gradient Algorithm (AdaGrad)**: maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems).\n",
        "\n",
        "- **Root Mean Square Propagation (RMSProp)**: maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing)\n",
        "\n",
        "Adam realizes the benefits of both AdaGrad and RMSProp. It requires only first-order gradients with little memory allocation. The method works by computing individual adaptive learning rates for different parameters from estimates of 1st and 2nd moments of gradients.\n",
        "\n",
        "Citation: https://arxiv.org/abs/1412.6980 \n",
        "\n",
        "For loss, we are calculating it using the **Cross Entropy** function. Entropy in machine learning is the number of bits required to transmit a randomly selected event from a probability distribution. A skewed distribution in this case would have a lower entropy than an even distribution. Cross-entropy builds upon the idea of entropy from information theory and calculates the number of bits required to represent or transmit an average event from one distribution compared to another distribution.\n",
        "\n",
        "Cross entropy is widely used in classification problems.\n",
        "\n",
        "Each example has a known class label with a probability of 1.0, and a probability of 0.0 for all other labels. A model can estimate the probability of an example belonging to each class label. Cross-entropy can then be used to calculate the difference between the two probability distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zV09dQCgcn_4"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def get_optimizer(model):\n",
        "  return optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKl5ddS7crhp"
      },
      "source": [
        "# Train Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ4xry4iIAX3"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljaolBjOFL9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df52f7cc-280e-44e8-80c4-263a86761ab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([256, 3, 32, 32])\n",
            "Shape of y: torch.Size([256]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "  print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "  print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch, (X, y) in enumerate(test_dataloader):\n",
        "  print(X.size())\n",
        "  print(y)\n",
        "  print(y.size())\n",
        "  break"
      ],
      "metadata": {
        "id": "mxLkcabbyUHL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e437a9-0254-43dc-b092-20f7ddc7eb5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 3, 32, 32])\n",
            "tensor([16,  1, 38, 33, 11, 38, 18, 12, 25, 35, 12,  7, 23,  7,  4,  9, 21, 20,\n",
            "        27, 38,  4, 33,  9,  3,  1, 11, 13, 10,  9, 11,  5, 17, 34, 23,  2, 17,\n",
            "         3, 12, 16,  8,  7, 30, 18, 12, 24, 25,  3, 10, 18,  8, 25, 13, 15,  9,\n",
            "        13, 35,  5, 26,  9, 16, 38, 10,  4,  9, 15,  9, 26,  2,  5, 28, 11, 25,\n",
            "        30, 34,  5, 12,  1, 10, 25, 25, 21, 33, 25,  7, 10, 35,  3,  7, 22, 13,\n",
            "         3,  1,  2, 14, 12, 32,  3, 38,  9, 33,  1, 10,  5, 11, 33,  4, 35, 25,\n",
            "        33,  4,  1, 14, 16, 10, 30,  3, 27, 29,  1, 17, 13,  7,  1,  8,  2, 10,\n",
            "        10, 30,  1,  6, 36,  3, 14, 13, 11, 10, 18, 40,  2, 38, 41,  4,  6, 18,\n",
            "        17, 25,  2, 41, 11, 21,  7, 24, 11, 25, 17,  3,  6,  9,  7,  4, 13, 16,\n",
            "         4, 25, 18,  9, 13, 14, 29, 17, 13, 38, 26, 25, 33,  1,  3, 40, 13,  2,\n",
            "         8,  4, 36, 25, 20, 25, 18,  1, 10,  8, 10, 29, 12, 38, 31,  2,  8, 38,\n",
            "        18, 28, 17,  9,  4,  1, 17,  9,  2, 31, 13, 15, 15, 38, 25,  5, 25, 13,\n",
            "        10,  5,  4, 10,  2,  4,  5,  1, 14, 12, 12,  5,  8, 36, 25, 13, 33, 18,\n",
            "        33, 19, 12, 30,  4, 18, 12, 13, 20,  0, 10, 40,  5,  8, 12, 38, 20, 14,\n",
            "         0, 36, 34, 28])\n",
            "torch.Size([256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(5, 3)\n",
        "print(a)\n",
        "b = (torch.argmax(a, 1) == torch.tensor([2, 1, 0, 1, 2]))\n",
        "print(b)\n",
        "b.int()\n",
        "\n",
        "print(testset[0][0].size())\n",
        "\n",
        "c = torch.rand(4, 3, 299, 299).cuda()\n",
        "print(c.size())\n",
        "d = model(c)\n",
        "print(d)\n",
        "torch.argmax(d, 1)"
      ],
      "metadata": {
        "id": "8JBi_cCpwsmF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "outputId": "42e24c3a-d050-4f4a-d3c8-d1c3783cfd76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9990, 0.3114, 0.1948],\n",
            "        [0.3532, 0.4792, 0.8535],\n",
            "        [0.9818, 0.3584, 0.8431],\n",
            "        [0.6719, 0.8854, 0.1372],\n",
            "        [0.0816, 0.2817, 0.8904]])\n",
            "tensor([False, False,  True,  True,  True])\n",
            "torch.Size([3, 32, 32])\n",
            "torch.Size([4, 3, 299, 299])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-8f576d686843>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-eb6ddd4590f9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdqYIk6kH1A8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  running_loss = 0\n",
        "\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    y_pred = model(X)\n",
        "\n",
        "    loss = loss_fn(y_pred, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss = loss.item()\n",
        "\n",
        "    running_loss += loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      current = batch * len(X)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "  return running_loss / size\n",
        "\n",
        "\n",
        "def eval(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  \n",
        "  model.eval()\n",
        "\n",
        "  test_loss, correct = 0, 0\n",
        "  running_loss = []\n",
        "\n",
        "  f1, prec, rec = 0.0, 0.0, 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      y_pred = model(X)\n",
        "\n",
        "      test_loss += loss_fn(y_pred, y).item()\n",
        "      correct += (torch.argmax(y_pred, 1) == y).sum().item()\n",
        "\n",
        "      y = y.cpu()\n",
        "\n",
        "      y_pred = y_pred.cpu()\n",
        "      y_pred = torch.argmax(y_pred, 1)\n",
        "\n",
        "      f1 += f1_score(y, y_pred, average='weighted')\n",
        "      prec += precision_score(y, y_pred, average='weighted')\n",
        "      rec += recall_score(y, y_pred, average='weighted')\n",
        "  \n",
        "      running_loss.append(test_loss)\n",
        "\n",
        "    test_loss = test_loss / num_batches\n",
        "    correct = correct / size\n",
        "    acc = 100 * correct\n",
        "\n",
        "    f1 = f1 / num_batches\n",
        "    prec = prec / num_batches\n",
        "    rec = rec / num_batches\n",
        "\n",
        "    print(f\"Accuracy: {acc:>0.1f}%, Avg loss per batch: {test_loss:>8f} \\n\")\n",
        "    print(f\"Avg F1 Score: {f1:>8f} \\n\")\n",
        "    print(f\"Avg Precision Score: {prec:>8f} \\n\")\n",
        "    print(f\"Avg Recall Score: {rec:>8f} \\n\")\n",
        "\n",
        "    return running_loss\n",
        "\n",
        "\n",
        "def train_model(model):\n",
        "  optimizer = get_optimizer(model)\n",
        "  model = model.to(device)\n",
        "  \n",
        "  train_error = []\n",
        "  running_train_loss = 0\n",
        "\n",
        "  for t in range(EPOCHS): \n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "\n",
        "    loss = train(train_dataloader, model, loss_fn, optimizer)\n",
        "    train_error.append(loss)\n",
        "\n",
        "    print(\"Validation set:\")\n",
        "    eval(val_dataloader, model, loss_fn)\n",
        "\n",
        "  print(\"Done!\")\n",
        "\n",
        "  return train_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMgu07Rmc0MF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "4862a614-15ea-425a-f6de-898f0ae83294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting: lenet_bn\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 3.935223  [    0/31367]\n",
            "loss: 1.025669  [25600/31367]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-8aa485f975e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Starting: {model_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mtrain_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-c699aff7d7c4>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0mtrain_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-c699aff7d7c4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f3fbba8c31b1>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m       \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfn_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfn_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mfn_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbrightness_factor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1249\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_brightness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrightness_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mfn_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontrast_factor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "models_leresse = {\n",
        "    'lenet_bn': LeNet_BN(),\n",
        "    'lenet': LeNet(),\n",
        "    'non_resnet': ResNet(use_resnet=False),\n",
        "    'resnet': ResNet(),\n",
        "    'nonresnet_senet_last_sigmoid': SEAndResNet(use_resnet=False),\n",
        "    'nonresnet_senet_last_relu': SEAndResNet(use_resnet=False, last_activation=nn.ReLU),\n",
        "    'senet_and_resnet_last_sigmoid': SEAndResNet(),\n",
        "    'senet_and_resnet_last_relu': SEAndResNet(last_activation=nn.ReLU),\n",
        "}\n",
        "\n",
        "for (model_name, model) in models.items():\n",
        "  print(f'Starting: {model_name}')\n",
        "  train_error = train_model(model)\n",
        "\n",
        "  plt.plot(np.linspace(1, EPOCHS, EPOCHS, dtype=int), train_error, label=model_name)\n",
        "  \n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss')\n",
        "plt.legend(loc='upper center')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3Lcrcs1SHlO"
      },
      "source": [
        "Now, running each model on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuCUl9aXbALC"
      },
      "outputs": [],
      "source": [
        "for (model_name, model) in models_leresse.items():\n",
        "  print(f'Starting: {model_name}')\n",
        "  test_loss = eval(test_dataloader, model, loss_fn)\n",
        "  plt.plot(np.linspace(1, len(test_loss), len(test_loss), dtype=int), test_loss, label=model_name)\n",
        "  \n",
        "plt.xlabel('Batch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss')\n",
        "plt.legend(loc='upper center')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for (model_name, model) in models_leresse.items():\n",
        "  print(model_name)\n",
        "  fig = plt.figure(figsize=(32, 32))\n",
        "  model.eval()\n",
        "  cnt = 0\n",
        "  with torch.no_grad():\n",
        "      for images, labels in test_dataloader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          outputs = model(images)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          label = []\n",
        "          image = []\n",
        "          for pred in predicted:\n",
        "            pred = pred.cpu().item()\n",
        "            label.append(classes[pred])\n",
        "            cnt += 1\n",
        "            if (cnt > 12):\n",
        "              break\n",
        "          cnt = 0\n",
        "          for img in images:\n",
        "              img = img.cpu()\n",
        "              i = img.numpy()\n",
        "              i = np.transpose(i, (1, 2, 0))\n",
        "              i = i * data_std + data_mean\n",
        "              image.append(i)\n",
        "              cnt += 1\n",
        "              if (cnt > 12):\n",
        "                break\n",
        "          break\n",
        "  for i in range(10):\n",
        "    fig.add_subplot(1, 10, i+1)\n",
        "    plt.title(label[i])\n",
        "    plt.imshow(image[i])\n",
        "  plt.show()\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "JeswoLgvf_Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KucHGZQmbIRj"
      },
      "source": [
        "# ResNet with Transfer Learning\n",
        "We use ResNet50 deep learning model as the pre-trained model for feature extraction for Transfer Learning.\n",
        "\n",
        "We are fine-tuning the pre-trained model on the GTSRB dataset by unfreezing the final layers of the network and retraining them on the new dataset. This allows the model to learn new, task-specific features that are relevant to the GTSRB dataset, while still leveraging the knowledge learned by the pre-trained model on the larger dataset. This can help to improve the performance of the model on the GTSRB dataset and can save time and computational resources compared to training the model from scratch.\n",
        "\n",
        "The code modifies an existing pre-trained ResNet model by removing the final layer, flattening the output from the convolutional base, and then adding several more fully-connected layers on top.\n",
        "\n",
        "The first added layer is a batch normalization layer, which normalizes the activations of the previous layer. This can help improve the training of the model by making the distribution of each activation more similar, which can make it easier for the optimizer to find good parameter values.\n",
        "\n",
        "The next added layer is a linear layer with 512 units, followed by a ReLU nonlinearity, which applies the element-wise function max(0, x) to the activations. This is a common choice for the activation function in a neural network because it introduces nonlinearity, allowing the model to learn a more complex mapping from inputs to outputs.\n",
        "\n",
        "The model then includes a dropout layer with a dropout rate of 0.45, which randomly sets a proportion of the activations of the previous layer to zero. This can help prevent overfitting by making the model less dependent on any one activation and forcing it to learn a more general representation of the data.\n",
        "\n",
        "The model then includes another batch normalization layer, followed by a final linear layer that outputs the predicted class probabilities for each input. This final layer uses a log-softmax activation function, which is equivalent to applying the softmax function and then taking the natural logarithm of the result. This is often used in classification tasks because it allows for the prediction of probabilities, but with a more numerically stable implementation than using the raw softmax output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fp6JDuhQbKt6"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms, models\n",
        "\n",
        "# Test Accuracy: 75.1%\n",
        "# BATCH_SIZE = 256\n",
        "# EPOCHS = 15\n",
        "# LEARNING_RATE = 0.0001\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "# for resnet pre-trained\n",
        "train_dataloader = DataLoader(trainset_rpt, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(valset_rpt, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = DataLoader(testset_rpt, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPbCor1Xxe-V"
      },
      "outputs": [],
      "source": [
        "# load pre-trained model\n",
        "model = torchvision.models.resnet18(pretrained=True).to(device)\n",
        "\n",
        "# freeze layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_classes=43\n",
        "num_ftrs = model.fc.in_features\n",
        "\n",
        "model = nn.Sequential(\n",
        "          *list(model.children())[:-1],\n",
        "          nn.Flatten(),\n",
        "          nn.BatchNorm1d(num_ftrs),\n",
        "          nn.Linear(num_ftrs, 512),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.Dropout(0.45),\n",
        "          nn.BatchNorm1d(512),\n",
        "          nn.Linear(512, num_classes),\n",
        "          nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "list(model.children())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMHXj8cy6ESN"
      },
      "source": [
        "One reason why the test set saw lower accuracy is because of the split of the data. \n",
        "In general, having a large amount of data in the test set relative to the size of the training set can make it more difficult for a model to achieve high accuracy on the test set. This is because a model is typically trained on the training set in order to learn the patterns in the data that are relevant for making accurate predictions. In most cases, the split of data should be 80% training, 10% validation, and 10% testing. However, in this case, the way the datasets were already pre-processed, left a lot more data in the test set than would be optimal to achieve the best model. In the case of a pre-trained model that relies on the validation set for fine-tuning, the ratio becomes important to cover as many test cases as possible. However, the validation to testing ratio was almost 1:2, making it difficult to generalize to the testing cases, thus lowering its accuracy in comparison with validation set.\n",
        "\n",
        "Another reason transfer learning's accuracy rate is lower is likely due to the fact that it is pretrained on `ImageNet` data, whose classifications do not reflect the classification depth we need for traffic signals. Because the set of data it is trained on is so differently recognized and classified in comparison with GTSRB, it is better to train it from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajjB3zVIxYWe"
      },
      "outputs": [],
      "source": [
        "trained = train_model(model)\n",
        "eval(test_dataloader, model, loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaTiLxKJSxm4"
      },
      "source": [
        "# Inception Net\n",
        "In an image classification task, the size of the salient feature can considerably vary within the image frame. Hence, deciding on a fixed kernel size is rather difficult. Larger kernels are preferred for more global features that are distributed over a large area of the image, on the other hand, smaller kernels provide good results in detecting area-specific features that are distributed across the image frame. For effective recognition of traffic signals while driving, it makes sense to implement a neural network capable of these variations. Instead of simply going deeper in terms of the number of layers, it goes wider with kernels of different sizes. Multiple kernels of different sizes are implemented within the same layer.\n",
        "\n",
        "We will be implementing a variation of GoogLeNet, one of the simplest forms of Inception Net.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1CmymDqS3Uf"
      },
      "outputs": [],
      "source": [
        "# for inception net pre-trained\n",
        "train_dataloader = DataLoader(trainset_inc, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(valset_inc, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = DataLoader(testset_inc, batch_size=BATCH_SIZE, shuffle=False)\n",
        "# Current Test Accuracy: \n",
        "\n",
        "model = models.inception_v3(pretrained=True).to(device)\n",
        "\n",
        "for parameter in model.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "num_classes=43\n",
        "\n",
        "# Handle the auxilary net\n",
        "num_ftrs = model.AuxLogits.fc.in_features\n",
        "model.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "# Handle the primary net\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(          \n",
        "          nn.Flatten(),\n",
        "          nn.BatchNorm1d(num_ftrs),\n",
        "          nn.Linear(num_ftrs, 512),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.Dropout(0.45),\n",
        "          nn.BatchNorm1d(512),\n",
        "          nn.Linear(512, num_classes),\n",
        "          nn.LogSoftmax(dim=1)\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF6PjhNmwxZW"
      },
      "source": [
        "Because Inception Net outputs 2 tensors and only one tensor is passed into the loss function as `y_pred`, we need to make sure that the first argument you pass to the cross_entropy_loss function is a Tensor that has the right shape and format. The two output tensors Inception Net gives are `Logits` and `AuxLogits`. The key difference between logits and auxiliary logits is that logits are used to make the final predictions for the model, while auxlogits are used to provide additional information that can help improve the accuracy of the model. For example, in the Inception network, the auxlogits are the outputs of intermediate layers of the network that are fed into the final prediction layer, along with the logits, to make the final predictions. This allows the model to make more accurate predictions by incorporating information from multiple layers of the network. the tensor we want is the `logits` tensor. So we modify our training function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu5TXjhWwwJs"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  running_loss = 0\n",
        "\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    y_pred = model(X)\n",
        "\n",
        "    loss = loss_fn(y_pred.logits, y) # change\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss = loss.item()\n",
        "\n",
        "    running_loss += loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      current = batch * len(X)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "  return running_loss / size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj2QYVk525Ky"
      },
      "outputs": [],
      "source": [
        "trained = train_model(model)\n",
        "eval(test_dataloader, model, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval(test_dataloader, model, loss_fn)"
      ],
      "metadata": {
        "id": "AfcHmLXzujHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inception Net GoogLeNetV3\n",
        "\n",
        "GoogLeNet V3 is a convolutional neural network (CNN) that uses a combination of convolutional layers and inception modules to extract features from input data. The model takes in a tensor with a specified number of channels as input and processes it through a series of convolutional and pooling layers to reduce its size and increase the number of channels. This is followed by a series of inception modules that learn to combine features from different spatial sizes and aspect ratios. The model then uses another series of convolutional and pooling layers to further reduce the size of the feature map and increase the number of channels. Finally, the model uses a fully-connected layer to make predictions based on the extracted features."
      ],
      "metadata": {
        "id": "lQQkqquOIHcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inception Net\n",
        "class GoogLeNetV3(nn.Module):\n",
        "    def __init__(self, channels_in):\n",
        "        super(GoogLeNetV3, self).__init__()\n",
        "        self.in_block = nn.Sequential(\n",
        "            Conv2d_BN(channels_in, 32, 3, stride=2, padding=1),  # size /= 2\n",
        "            Conv2d_BN(32, 32, 3, stride=1, padding=1),\n",
        "            Conv2d_BN(32, 64, 3, stride=1, padding=1),\n",
        "            nn.MaxPool2d(3, stride=2, padding=1),  # size /= 2\n",
        "            Conv2d_BN(64, 80, 1, stride=1, padding=0),\n",
        "            Conv2d_BN(80, 192, 3, stride=1, padding=1),\n",
        "            nn.MaxPool2d(3, stride=2, padding=1)  # size /= 2\n",
        "        )  # 192 channels\n",
        "        self.mix_block = nn.Sequential(\n",
        "            InceptionA(192, 32),\n",
        "            InceptionA(256, 64),\n",
        "            InceptionA(288, 64),\n",
        "            InceptionB(288),  # size /= 2\n",
        "            InceptionC(768, 128),\n",
        "            InceptionC(768, 160),\n",
        "            InceptionC(768, 160),\n",
        "            InceptionC(768, 192),\n",
        "            InceptionD(768),  # size /= 2\n",
        "            InceptionE(1280),\n",
        "            InceptionE(2048)\n",
        "        )  # 2048 channels\n",
        "        self.out_block = nn.Sequential(\n",
        "            Conv2d_BN(2048, 1024, 1, stride=1, padding=0),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "        )  # 1024 channels\n",
        "        self.full_connect = nn.Linear(1024, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.in_block(x)\n",
        "        x = self.mix_block(x)\n",
        "        x = self.out_block(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.full_connect(x)\n",
        "\n",
        "class Conv2d_BN(nn.Module):\n",
        "    def __init__(self, channels_in, channels_out, kernel_size, padding, stride=1, acti=nn.LeakyReLU(0.2, inplace=True)):\n",
        "        super(Conv2d_BN, self).__init__()\n",
        "        self.conv2d_bn = nn.Sequential(\n",
        "            nn.Conv2d(channels_in, channels_out, kernel_size, stride, padding, bias=False),\n",
        "            nn.BatchNorm2d(channels_out),\n",
        "            acti\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv2d_bn(x)\n",
        "\n",
        "class InceptionA(nn.Module):\n",
        "    def __init__(self, channels_in, pool_channels):\n",
        "        super(InceptionA, self).__init__()\n",
        "        self.branch1x1 = Conv2d_BN(channels_in, 64, 1, stride=1, padding=0)  # 64 channels\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            Conv2d_BN(channels_in, 48, 1, stride=1, padding=0),\n",
        "            Conv2d_BN(48, 64, 5, stride=1, padding=2)\n",
        "        )  # 64 channels\n",
        "        self.branch3x3dbl = nn.Sequential(\n",
        "            Conv2d_BN(channels_in, 64, 1, stride=1, padding=0),\n",
        "            Conv2d_BN(64, 96, 3, stride=1, padding=1),\n",
        "            Conv2d_BN(96, 96, 3, stride=1, padding=1)\n",
        "        )  # 96 channels\n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.AvgPool2d(3, stride=1, padding=1),\n",
        "            Conv2d_BN(channels_in, pool_channels, 1, stride=1, padding=0)\n",
        "        )  # pool_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = [self.branch1x1(x), self.branch5x5(x), self.branch3x3dbl(x), self.branch_pool(x)]\n",
        "        # 64 + 64 + 96 + pool_channels\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "class InceptionB(nn.Module):\n",
        "    def __init__(self, channels_in):\n",
        "        super(InceptionB, self).__init__()\n",
        "        self.branch3x3 = Conv2d_BN(channels_in, 384, 3, stride=2, padding=1)  # 384 channels\n",
        "        self.branch3x3dbl = nn.Sequential(\n",
        "            Conv2d_BN(channels_in, 64, 1, padding=0),\n",
        "            Conv2d_BN(64, 96, 3, padding=1),\n",
        "            Conv2d_BN(96, 96, 3, stride=2, padding=1)\n",
        "        )  # 96 channels\n",
        "        self.branch_pool = nn.MaxPool2d(3, stride=2, padding=1)  # channels_in\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = [self.branch3x3(x), self.branch3x3dbl(x), self.branch_pool(x)]\n",
        "        # 384 + 96 + channels_in\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "class InceptionC(nn.Module):\n",
        "    def __init__(self, channels_in, channels_7x7):\n",
        "        super(InceptionC, self).__init__()\n",
        "        self.branch1x1 = Conv2d_BN(channels_in, 192, 1, stride=1, padding=0)  # 192 channels\n",
        "        self.branch7x7 = nn.Sequential(\n",
        "            Conv2d_BN(channels_in, channels_7x7, 1, stride=1, padding=0),\n",
        "            Conv2d_BN(channels_7x7, channels_7x7, (1, 7), stride=1, padding=(0, 3)),\n",
        "            Conv2d_BN(channels_7x7, 192, (7, 1), stride=1, padding=(3, 0))\n",
        "        )  # 192 channels\n",
        "        self.branch7x7dbl = nn.Sequential(\n",
        "            Conv2d_BN(channels_in, channels_7x7, 1, stride=1, padding=0),\n",
        "            Conv2d_BN(channels_7x7, channels_7x7, (7, 1), stride=1, padding=(3, 0)),\n",
        "            Conv2d_BN(channels_7x7, channels_7x7, (1, 7), stride=1, padding=(0, 3)),\n",
        "            Conv2d_BN(channels_7x7, channels_7x7, (7, 1), stride=1, padding=(3, 0)),\n",
        "            Conv2d_BN(channels_7x7, 192, (1, 7), stride=1, padding=(0, 3))\n",
        "        )  # 192 channels\n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.AvgPool2d(3, stride=1, padding=1),\n",
        "            Conv2d_BN(channels_in, 192, 1, stride=1, padding=0)\n",
        "        )  # 192 channels\n",
        "    \n",
        "    def forward(self, x):\n",
        "        outputs = [self.branch1x1(x), self.branch7x7(x), self.branch7x7dbl(x), self.branch_pool(x)]\n",
        "        # 192 + 192 + 192 + 192 = 768 channels\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "class InceptionD(nn.Module):\n",
        "    def __init__(self, channels_in):\n",
        "        super(InceptionD, self).__init__()\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            Conv2d_BN(channels_in, 192, 1, stride=1, padding=0),\n",
        "            Conv2d_BN(192, 320, 3, stride=2, padding=1)\n",
        "        )  # 320 channels\n",
        "        self.branch7x7x3 = nn.Sequential(\n",
        "            Conv2d_BN(channels_in, 192, 1, stride=1, padding=0),\n",
        "            Conv2d_BN(192, 192, (1, 7), stride=1, padding=(0, 3)),\n",
        "            Conv2d_BN(192, 192, (7, 1), stride=1, padding=(3, 0)),\n",
        "            Conv2d_BN(192, 192, 3, stride=2, padding=1)\n",
        "        )  # 192 chnnels\n",
        "        self.branch_pool = nn.MaxPool2d(3, stride=2, padding=1)  # channels_in\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = [self.branch3x3(x), self.branch7x7x3(x), self.branch_pool(x)]\n",
        "        # 320 + 192 + channels_in\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "class InceptionE(nn.Module):\n",
        "    def __init__(self, channels_in):\n",
        "        super(InceptionE, self).__init__()\n",
        "        self.branch1x1 = Conv2d_BN(channels_in, 320, 1, stride=1, padding=0)  # 320 channels\n",
        "\n",
        "        self.branch3x3_1 = Conv2d_BN(channels_in, 384, 1, stride=1, padding=0)\n",
        "        self.branch3x3_2a = Conv2d_BN(384, 384, (1, 3), stride=1, padding=(0, 1))\n",
        "        self.branch3x3_2b = Conv2d_BN(384, 384, (3, 1), stride=1, padding=(1, 0))\n",
        "        # 768 channels\n",
        "\n",
        "        self.branch3x3dbl_1 = nn.Sequential(\n",
        "            Conv2d_BN(channels_in, 448, 1, stride=1, padding=0),\n",
        "            Conv2d_BN(448, 384, 3, stride=1, padding=1)\n",
        "        )\n",
        "        self.branch3x3dbl_2a = Conv2d_BN(384, 384, (1, 3), stride=1, padding=(0, 1))\n",
        "        self.branch3x3dbl_2b = Conv2d_BN(384, 384, (3, 1), stride=1, padding=(1, 0))\n",
        "        # 768 channels\n",
        "        \n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.AvgPool2d(3, stride=1, padding=1),\n",
        "            Conv2d_BN(channels_in, 192, 1, stride=1, padding=0)\n",
        "        )  # 192 channels\n",
        "    \n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = torch.cat([self.branch3x3_2a(branch3x3), self.branch3x3_2b(branch3x3)], 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = torch.cat([self.branch3x3dbl_2a(branch3x3dbl), self.branch3x3dbl_2b(branch3x3dbl)], 1)\n",
        "\n",
        "        branch_pool = self.branch_pool(x)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        # 320 + 768 + 768 + 192 = 2048 channels\n",
        "        return torch.cat(outputs, 1)"
      ],
      "metadata": {
        "id": "vVX_xXOAFfQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TF7eKv1eW7YH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "ln3z9vwcc5g9"
      ]
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}